{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TIU_tensor_sp_transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hutiLlgFAlu"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipefjH6Fw4P"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsgjUDF4FzOo",
        "outputId": "9329dc8c-b45b-4f75-ed51-917b453b2b3e"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oed7D13zF2qN"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/dataset/TIU/twitter/sentencepiece/original1217_train.csv\", header=None)\n",
        "tiu_input = df[0].to_list()\n",
        "tiu_output = df[1].to_list()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIyKhrFUF89U",
        "outputId": "e4e2e53a-02f2-4ce4-f4b0-287177e6aa5a"
      },
      "source": [
        "tiu_input[0:2]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['なwwwにwwwこwwwれwww', '日本語上手ですね? 学べましたか?。']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOrMmpJbOMW4",
        "outputId": "7c536a0c-c2a5-41c6-fd7b-c6c579d1c6b3"
      },
      "source": [
        "!pip install sentencepiece > /dev/null\n",
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"/content/drive/MyDrive/dataset/TIU/twitter/sentencepiece/sp_tiu1217.model\")\n",
        "sp.encode(\"朝青龍になりたい\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 487, 945, 2939, 4055]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSfmexedMVa0",
        "outputId": "0b43de58-801b-45e8-b778-2f4f6b46d2a5"
      },
      "source": [
        "print(sp.get_piece_size())\n",
        "sp.id_to_piece([0,1,2,3,4,5])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<s>', '</s>', '。', ':', '、']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5RrStcwMEXV"
      },
      "source": [
        "VOCAB_SIZE_INPUT=sp.get_piece_size()\n",
        "VOCAB_SIZE_OUTPUT=sp.get_piece_size()\n",
        "MAX_LENGTH = 20"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRqqN3o2Iq-j"
      },
      "source": [
        "inputs = [[1] + sp.encode(sentence) + [2]\n",
        "          for sentence in tiu_input]\n",
        "outputs = [[1] + sp.encode(sentence) + [2]\n",
        "          for sentence in tiu_output]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeedj3KGOYtO",
        "outputId": "b7738262-029b-4c6a-e245-963f376ec998"
      },
      "source": [
        "print(len(inputs), len(outputs))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158351 158351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTaFxJkeOkIP",
        "outputId": "d2906f0b-7a3d-46c8-c3f2-2dc0cbe9eb53"
      },
      "source": [
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent)>MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent)>MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "  \n",
        "print(len(inputs), len(outputs))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "133915 133915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov8iJyHfPnWr"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfda_6TPp-Vh"
      },
      "source": [
        "TEST_SIZE=1024\n",
        "inputs = inputs[:-1*TEST_SIZE]\n",
        "outputs = outputs[:-1*TEST_SIZE]\n",
        "inputs_test = inputs[-1*TEST_SIZE:]\n",
        "outputs_test = outputs[-1*TEST_SIZE:]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f15fzH9LQFg8"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((inputs_test, outputs_test))\n",
        "dataset_test = dataset_test.cache()\n",
        "dataset_test = dataset_test.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnER0z_fHa3v",
        "outputId": "d873ad51-45d1-4c2d-8383-ee5272db3ade"
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((inputs_test, outputs_test))\n",
        "dataset_test = dataset_test.cache()\n",
        "dataset_test = dataset_test.shuffle(2000).batch(BATCH_SIZE)\n",
        "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "for (batch, (enc_inputs, targets)) in enumerate(dataset_test):\n",
        "  print(batch)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVnLnPKuQMo4"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "  def get_angles(self, pos, i, d_model):\n",
        "    angles = 1/np.power(10000., (2*(i//2))/np.float32(d_model)  )\n",
        "    return pos*angles #[seq_length, d_model]\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = inputs.shape.as_list()[-1]\n",
        "    angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis, :],\n",
        "                             d_model)\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    pos_encoding = angles[np.newaxis, ...]\n",
        "    return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrZXOv_8aJN3"
      },
      "source": [
        "def scaled_dot_product_attention(quaries, keys, values, mask):\n",
        "  product = tf.matmul(quaries, keys, transpose_b=True)\n",
        "\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask*-1e9)\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values  )\n",
        "\n",
        "  return attention"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixo1dEBSaLYz"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "    self.quary_lin = layers.Dense(units=self.d_model)\n",
        "    self.key_lin = layers.Dense(units=self.d_model)\n",
        "    self.value_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "  def split_proj(self, inputs, batch_size): # inputs: [batch_size, seq_length, nb_proj, d_proj]\n",
        "    shape = (batch_size,\n",
        "             -1,\n",
        "             self.nb_proj,\n",
        "             self.d_proj)\n",
        "    splited_inputs = tf.reshape(inputs, shape=shape) #[batch_size, seq_length, nb_proj, d_proj]\n",
        "    return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) #[batch_size, nb_proj, seq_length, d_proj]\n",
        "\n",
        "  def call(self, quaries, keys, values, mask):\n",
        "    batch_size = tf.shape(quaries)[0]\n",
        "\n",
        "    quaries = self.quary_lin(quaries)\n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values)\n",
        "\n",
        "    quaries = self.split_proj(quaries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)\n",
        "    values = self.split_proj(values, batch_size)\n",
        "\n",
        "    attention = scaled_dot_product_attention(quaries, keys, values, mask)\n",
        "    attention = tf.transpose(attention, perm=[0,2,1,3])\n",
        "    concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "    outputs = self.final_lin(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qioHDY7SaNQY"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "  def __init__(self, FFN_units, nb_proj, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    attention = self.multi_head_attention(inputs,\n",
        "                                          inputs,\n",
        "                                          inputs,\n",
        "                                          mask)\n",
        "    attention = self.dropout_1(attention, training=training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    outputs = self.dense_1(attention)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_2(outputs)\n",
        "    outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwxHAHlBaPir"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout, \n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name='encoder'):\n",
        "    super(Encoder, self).__init__(name=name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout)\n",
        "    self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                   nb_proj,\n",
        "                                   dropout)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outpus = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk1gIwflaRp_"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "  def __init__(self, FFN_units, nb_proj, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    \n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense_1  = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "  \n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    attention = self.multi_head_attention_1(inputs, \n",
        "                                           inputs,\n",
        "                                           inputs,\n",
        "                                           mask_1)\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.norm_1(attention+inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,\n",
        "                                              enc_outputs,\n",
        "                                              enc_outputs,\n",
        "                                              mask_2)\n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "    outputs= self.dense_1(attention_2)\n",
        "    outputs= self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.norm_3(outputs+attention_2)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOQQNvdbaTyL"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "  def __init__(self, nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name='decoder'):\n",
        "    super(Decoder, self).__init__(name=name)\n",
        "    self.d_model=d_model\n",
        "    self.nb_layers = nb_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                   nb_proj,\n",
        "                                   dropout)\n",
        "                        for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) \n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs,\n",
        "                                  enc_outputs,\n",
        "                                  mask_1,\n",
        "                                  mask_2,\n",
        "                                  training)\n",
        "    return outputs"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCh4vkKSaVos"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               vocab_size_enc,\n",
        "               vocab_size_dec,\n",
        "               d_model,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               name='transformer'):\n",
        "    super(Transformer, self).__init__(name=name)\n",
        "\n",
        "    self.encoder = Encoder(nb_layers,\n",
        "                           FFN_units,\n",
        "                           nb_proj,\n",
        "                           dropout,\n",
        "                           vocab_size_enc,\n",
        "                           d_model)\n",
        "    self.decoder = Decoder(nb_layers,\n",
        "                           FFN_units,\n",
        "                           nb_proj,\n",
        "                           dropout,\n",
        "                           vocab_size_dec,\n",
        "                           d_model)\n",
        "    self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "  \n",
        "  def create_padding_mask(self, seq): #seq:[batch_size, seq_length]\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  def create_look_ahead_mask(self, seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask\n",
        "\n",
        "  def call(self, enc_inputs, dec_inputs, training):\n",
        "    enc_mask = self.create_padding_mask(enc_inputs)\n",
        "    dec_mask_1 = tf.maximum(\n",
        "        self.create_padding_mask(dec_inputs),\n",
        "        self.create_look_ahead_mask(dec_inputs)\n",
        "    )\n",
        "    dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "    enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "    dec_outputs = self.decoder(dec_inputs, \n",
        "                               enc_outputs,\n",
        "                               dec_mask_1,\n",
        "                               dec_mask_2,\n",
        "                               training)\n",
        "    outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcXPdbypaXnV",
        "outputId": "67316b29-b788-4b5b-a553-006c9a18c25c"
      },
      "source": [
        "def create_padding_mask(seq): #seq:[batch_size, seq_length]\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask\n",
        "seq = tf.cast([[837,836,0,273,0,0,0,0]], tf.int32)\n",
        "tf.maximum(create_padding_mask(seq), create_look_ahead_mask(seq))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 8, 8), dtype=float32, numpy=\n",
              "array([[[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyR895eyaarK"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "D_MODEL = 128 #512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_INPUT,\n",
        "                          vocab_size_dec = VOCAB_SIZE_OUTPUT,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM7pOLbRac8Z"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "  loss_ = loss_object(target, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *=mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY6ymvuoahu7"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model=tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step*(self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                    beta_1=0.9,\n",
        "                                    beta_2=0.98,\n",
        "                                    epsilon=1e-9)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4zfmC2Rajv7"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/model/TIU/tensor_sp_transformer/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest checkpoint restored\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U_5M21nxa09_",
        "outputId": "34c17fe7-e635-4192-a8e1-059ee39f16f5"
      },
      "source": [
        "def test_function():\n",
        "  s=0\n",
        "  for (batch, (enc_inputs, targets)) in enumerate(dataset_test):\n",
        "    dec_inputs = targets[:, :-1]\n",
        "    dec_outputs_real = targets[:, 1:]\n",
        "    predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "    #loss = loss_function(dec_outputs_real, predictions)\n",
        "    \n",
        "    test_accuracy(dec_outputs_real, predictions)\n",
        "    s+=test_accuracy.result()\n",
        "  print(\"Test: Epoch {} Accuracy {:.4f}\".format(epoch+1, s/(TEST_SIZE/BATCH_SIZE)))\n",
        "\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Start number of epoch {}\".format(epoch+1))\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "    dec_inputs = targets[:, :-1]\n",
        "    dec_outputs_real = targets[:, 1:]\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "      loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    #WARNING回避で追加\n",
        "    #optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    optimizer.apply_gradients((grad, var) for (grad, var) in zip(gradients, transformer.trainable_variables) if grad is not None)\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "          epoch+1, batch, train_loss.result(), train_accuracy.result()\n",
        "      ))\n",
        "\n",
        "  test_function()\n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "\n",
        "  print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start number of epoch 1\n",
            "Epoch 1 Batch 0 Loss 2.8627 Accuracy 0.1859\n",
            "Epoch 1 Batch 100 Loss 2.5764 Accuracy 0.1875\n",
            "Epoch 1 Batch 200 Loss 2.5595 Accuracy 0.1887\n",
            "Epoch 1 Batch 300 Loss 2.5685 Accuracy 0.1882\n",
            "Epoch 1 Batch 400 Loss 2.5721 Accuracy 0.1882\n",
            "Epoch 1 Batch 500 Loss 2.5758 Accuracy 0.1879\n",
            "Epoch 1 Batch 600 Loss 2.5825 Accuracy 0.1878\n",
            "Epoch 1 Batch 700 Loss 2.5882 Accuracy 0.1875\n",
            "Epoch 1 Batch 800 Loss 2.5934 Accuracy 0.1872\n",
            "Epoch 1 Batch 900 Loss 2.5985 Accuracy 0.1867\n",
            "Epoch 1 Batch 1000 Loss 2.6058 Accuracy 0.1864\n",
            "Epoch 1 Batch 1100 Loss 2.6090 Accuracy 0.1862\n",
            "Epoch 1 Batch 1200 Loss 2.6122 Accuracy 0.1860\n",
            "Epoch 1 Batch 1300 Loss 2.6176 Accuracy 0.1859\n",
            "Epoch 1 Batch 1400 Loss 2.6198 Accuracy 0.1858\n",
            "Epoch 1 Batch 1500 Loss 2.6232 Accuracy 0.1854\n",
            "Epoch 1 Batch 1600 Loss 2.6263 Accuracy 0.1852\n",
            "Epoch 1 Batch 1700 Loss 2.6301 Accuracy 0.1851\n",
            "Epoch 1 Batch 1800 Loss 2.6311 Accuracy 0.1851\n",
            "Epoch 1 Batch 1900 Loss 2.6330 Accuracy 0.1850\n",
            "Epoch 1 Batch 2000 Loss 2.6352 Accuracy 0.1849\n",
            "Test: Epoch 1 Accuracy 0.2003\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/model/TIU/tensor_sp_transformer/ckpt-4\n",
            "Time taken for 1 epoch: 687.4129936695099 secs\n",
            "\n",
            "Start number of epoch 2\n",
            "Epoch 2 Batch 0 Loss 2.7715 Accuracy 0.1900\n",
            "Epoch 2 Batch 100 Loss 2.5504 Accuracy 0.1878\n",
            "Epoch 2 Batch 200 Loss 2.5593 Accuracy 0.1888\n",
            "Epoch 2 Batch 300 Loss 2.5688 Accuracy 0.1886\n",
            "Epoch 2 Batch 400 Loss 2.5697 Accuracy 0.1893\n",
            "Epoch 2 Batch 500 Loss 2.5715 Accuracy 0.1894\n",
            "Epoch 2 Batch 600 Loss 2.5755 Accuracy 0.1892\n",
            "Epoch 2 Batch 700 Loss 2.5751 Accuracy 0.1889\n",
            "Epoch 2 Batch 800 Loss 2.5794 Accuracy 0.1886\n",
            "Epoch 2 Batch 900 Loss 2.5823 Accuracy 0.1885\n",
            "Epoch 2 Batch 1000 Loss 2.5865 Accuracy 0.1883\n",
            "Epoch 2 Batch 1100 Loss 2.5868 Accuracy 0.1883\n",
            "Epoch 2 Batch 1200 Loss 2.5894 Accuracy 0.1882\n",
            "Epoch 2 Batch 1300 Loss 2.5910 Accuracy 0.1881\n",
            "Epoch 2 Batch 1400 Loss 2.5942 Accuracy 0.1880\n",
            "Epoch 2 Batch 1500 Loss 2.5946 Accuracy 0.1878\n",
            "Epoch 2 Batch 1600 Loss 2.5960 Accuracy 0.1878\n",
            "Epoch 2 Batch 1700 Loss 2.5965 Accuracy 0.1879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-e64b721c3d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdec_outputs_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_outputs_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-0aac290baad1>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, enc_inputs, dec_inputs, training)\u001b[0m\n\u001b[1;32m     47\u001b[0m                                \u001b[0mdec_mask_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                \u001b[0mdec_mask_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                                training)\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-6819820233cc>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, enc_outputs, mask_1, mask_2, training)\u001b[0m\n\u001b[1;32m     31\u001b[0m                                   \u001b[0mmask_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                   \u001b[0mmask_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                   training)\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-a1275d41dff3>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, enc_outputs, mask_1, mask_2, training)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                               mask_2)\n\u001b[1;32m     37\u001b[0m     \u001b[0mattention_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mattention_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# If some components of the shape got lost due to adjustments, fix that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m       raise ValueError(\n\u001b[1;32m   1239\u001b[0m           \u001b[0;34m\"Tensor's shape %s is not compatible with supplied shape %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;31m# `_tensor_shape` is declared and defined in the definition of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \"\"\"\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \"\"\"\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0m__slots__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;34m\"\"\"Creates a new Dimension with the given value.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF8wfbdVO06M",
        "outputId": "0909c182-69b3-4f9c-ae77-a9f89bc4bbda"
      },
      "source": [
        "transformer.load_weights('/content/drive/MyDrive/model/TIU/tensor_sp_transformer/my_checkpoint')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7faa800e6090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2PrQ-LpbDX6"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  inp_sentence = [1] + sp.encode(inp_sentence) + [2]\n",
        "  enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "  output = tf.expand_dims([1], axis=0)\n",
        "\n",
        "  for _ in range(MAX_LENGTH):\n",
        "    predictions = transformer(enc_input, output, False) # [1, seq_length, vocab_size_fr]\n",
        "    prediction = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "    if predicted_id == 2:\n",
        "      return tf.squeeze(output, axis=0)\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return (tf.squeeze(output, axis=0))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnGblLBI1oLP"
      },
      "source": [
        "def translate(sentence):\n",
        "  output = evaluate(sentence).numpy()\n",
        "  output =[i for i in output]\n",
        "  print(output)\n",
        "  predicted_sentence = sp.decode(output)\n",
        "  #predicted_sentence = sp.decode([i for i in output])\n",
        "  print(\"Input: {}\".format(sentence))\n",
        "  print(\"predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PTrq9O906tdI",
        "outputId": "5acc0cc9-4b0a-449c-fe12-fb2c9e963f8b"
      },
      "source": [
        "output = evaluate(\"朝に何した？\").numpy()\n",
        "output = [int(i) for i in output]\n",
        "sp.decode(output)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'朝派も夜派も'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUaJSAxf5JCK"
      },
      "source": [
        "with open('/content/drive/MyDrive/dataset/newsample.txt') as f:\n",
        "  data = f.readlines()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_XKJqZ5Rqfv",
        "outputId": "15ad666f-3414-4213-86d9-03cf7d58af83"
      },
      "source": [
        "for d in data:\n",
        "  output = evaluate(d).numpy()\n",
        "  output = [int(i) for i in output]\n",
        "  print('細野：', d[:-1])\n",
        "  print('タウ：', sp.decode(output))\n",
        "  print('----------------------------------------------------------')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "細野： お久しぶりです。\n",
            "タウ： お久しぶりです。お久しぶりです。\n",
            "----------------------------------------------------------\n",
            "細野： コロナが流行っていますね。\n",
            "タウ： ほんとですよ。。\n",
            "----------------------------------------------------------\n",
            "細野： ステーキ食べたい\n",
            "タウ： これ、焼き肉ですか?\n",
            "----------------------------------------------------------\n",
            "細野： ステーキ食べたい！\n",
            "タウ： え、私も食べたことないです。\n",
            "----------------------------------------------------------\n",
            "細野： ペ・ヨンジュン\n",
            "タウ： 了解です!よろしくお願いします。\n",
            "----------------------------------------------------------\n",
            "細野： ペ・ヨンジュンが好きですか？\n",
            "タウ： はい!ぜひお願いします。\n",
            "----------------------------------------------------------\n",
            "細野： なんて呼べばいいですか？\n",
            "タウ： まゆって呼んでください!私はなんてお呼びしたらいいですか?\n",
            "----------------------------------------------------------\n",
            "細野： 最近いいこと無いので励ましてください。\n",
            "タウ： おー!ご愁傷さまです。\n",
            "----------------------------------------------------------\n",
            "細野： なちゃんって呼ばないでください。\n",
            "タウ： りょうかいです!私はなんて呼べばいいですか?\n",
            "----------------------------------------------------------\n",
            "細野： りなちゃんとも呼ばないでください。\n",
            "タウ： りょうかいです!私はなんて呼べばいいですか?\n",
            "----------------------------------------------------------\n",
            "細野： りなちゃんとも呼ばないでください。\n",
            "タウ： りょうかいです!私はなんて呼べばいいですか?\n",
            "----------------------------------------------------------\n",
            "細野： パーティにりなちゃんは呼ばないでください。\n",
            "タウ： りょうかいです!\n",
            "----------------------------------------------------------\n",
            "細野： パーティにペ・ヨンジュンは呼ばないでください。\n",
            "タウ： 了解です。では、、、、、、、あ、りなちゃんって呼んでください\n",
            "----------------------------------------------------------\n",
            "細野： 最近、ポケモン不思議のダンジョンDXをはじめました。\n",
            "タウ： おー!おー!私も同じです!\n",
            "----------------------------------------------------------\n",
            "細野： 褒めて！\n",
            "タウ： おめでとです!\n",
            "----------------------------------------------------------\n",
            "細野： りなちゃん\n",
            "タウ： りょうかいです\n",
            "----------------------------------------------------------\n",
            "細野： りなちゃんも\n",
            "タウ： え、私とあざとってたよ\n",
            "----------------------------------------------------------\n",
            "細野： りなちゃん\n",
            "タウ： りょうかいです\n",
            "----------------------------------------------------------\n",
            "細野： りなちゃんも呼ばないで\n",
            "タウ： あ、私はなんて呼べばいいかな?\n",
            "----------------------------------------------------------\n",
            "細野： お誕生日会にりなちゃんもペ・ヨンジュンも呼ばないでください。\n",
            "タウ： 了解です。。。\n",
            "----------------------------------------------------------\n",
            "細野： ももももももももももももももももももももも\n",
            "タウ： もももももももももももももももももも\n",
            "----------------------------------------------------------\n",
            "細野： りなちゃん無視しようよ\n",
            "タウ： りょうかいです\n",
            "----------------------------------------------------------\n",
            "細野： ちゃんとちゃんこりなちゃんと食べなさい\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： パーティにみみちゃんは呼んでください\n",
            "タウ： 了解です!みみみちゃんって呼んでもいいですか?\n",
            "----------------------------------------------------------\n",
            "細野： 私は怒っています。\n",
            "タウ： あ、それな。\n",
            "----------------------------------------------------------\n",
            "細野： 喜怒哀楽をあらわにしたりなちゃん。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： 現世に無償降臨したりなちゃん。\n",
            "タウ： え、え?!それとも、私のこと?\n",
            "----------------------------------------------------------\n",
            "細野： ホソノとも呼ばないでください。\n",
            "タウ： いえ、私はまだあがってて草\n",
            "----------------------------------------------------------\n",
            "細野： マイキーちゃんとも呼ばないでください。\n",
            "タウ： 了解です!私はなんて呼べばいいですか?\n",
            "----------------------------------------------------------\n",
            "細野： りなちゃんとも呼ばないでください。\n",
            "タウ： りょうかいです!私はなんて呼べばいいですか?\n",
            "----------------------------------------------------------\n",
            "細野： ▁ももももももももももももももももももも\n",
            "タウ： もももももももももももももももももも\n",
            "----------------------------------------------------------\n",
            "細野： スモモも桃も、桃のうちですか？\n",
            "タウ： おー!おめでとー!\n",
            "----------------------------------------------------------\n",
            "細野： お祝いしてください\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： クソリプおじさんみたいですね。呼ばないでください。\n",
            "タウ： おばさん、こんにちは。\n",
            "----------------------------------------------------------\n",
            "細野： 私の名前は細野です。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： 怒怒\n",
            "タウ： いやいや、、、、、、、、、、、、、、、、\n",
            "----------------------------------------------------------\n",
            "細野： 怒怒怒\n",
            "タウ： いやいや、、、、、、、、、、、、、、、、\n",
            "----------------------------------------------------------\n",
            "細野： 私は怒って怒って怒っています。\n",
            "タウ： あ、もう、、、、、、、、、、。。\n",
            "----------------------------------------------------------\n",
            "細野： バーカ！アホ！\n",
            "タウ： はい!そうです!\n",
            "----------------------------------------------------------\n",
            "細野： 異教徒だ！焼け！\n",
            "タウ： 日曜日は休みです。\n",
            "----------------------------------------------------------\n",
            "細野： アルコ&amp;ピースのラジオが好きです。\n",
            "タウ： 今、3人目です。\n",
            "----------------------------------------------------------\n",
            "細野： 日々私たちが過ごしている日常は、実は奇跡の連続なのかもしれない。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： お誕生日ネタ多くないですか？\n",
            "タウ： そうです。。。\n",
            "----------------------------------------------------------\n",
            "細野： 弟さんの体調は大丈夫ですか？\n",
            "タウ： 大丈夫です。。。\n",
            "----------------------------------------------------------\n",
            "細野： 弟の体調は大丈夫ですか？\n",
            "タウ： 大丈夫です!。。\n",
            "----------------------------------------------------------\n",
            "細野： 彼女が二本の刀を持って出て行きました。\n",
            "タウ： え、私もそう!。\n",
            "----------------------------------------------------------\n",
            "細野： かよ許せねぇよ、、、。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： 「火の花」やね。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： 床をカンナで、削ります！\n",
            "タウ： ご愁傷さまです。\n",
            "----------------------------------------------------------\n",
            "細野： ちゃんちゃんこ着たりなちゃん可愛い。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： YouTuber\n",
            "タウ： これって誰ですか?\n",
            "----------------------------------------------------------\n",
            "細野： 交換可能です。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： トレカ所持しています。交換可能でしょうか？\n",
            "タウ： 可能です。dm伺います。\n",
            "----------------------------------------------------------\n",
            "細野： 俺のオーロンゲデッキと勝負しようぜ。\n",
            "タウ： うらやましいです\n",
            "----------------------------------------------------------\n",
            "細野： ペ・ヨンジュン\n",
            "タウ： 了解です!よろしくお願いします。\n",
            "----------------------------------------------------------\n",
            "細野： ななななー。ななななー。週七、木の実ナナ。\n",
            "タウ： それな。私もか、か、か、かっきちゃん。\n",
            "----------------------------------------------------------\n",
            "細野： どうぞ。\n",
            "タウ： おー!。。。。。。。。。。。。。。\n",
            "----------------------------------------------------------\n",
            "細野： こちらこそ。ふふふふ。ふふ。ふふふ。\n",
            "タウ： りょうかいです。\n",
            "----------------------------------------------------------\n",
            "細野： 最近は金属バットにハマっています。\n",
            "タウ： pcのマックですか?\n",
            "----------------------------------------------------------\n",
            "細野： 誕生日プレゼントに炬燵をください。\n",
            "タウ： おめでとうございます。。。。楽しみです。\n",
            "----------------------------------------------------------\n",
            "細野： お茶は、伊右衛門のお茶が好きなんだよね。俺。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： 値段は高くないですよ。150円ぐらいです。\n",
            "タウ： もう1回もおもしろいです。\n",
            "----------------------------------------------------------\n",
            "細野： debug\n",
            "タウ： それはそれでいいです。\n",
            "----------------------------------------------------------\n",
            "細野： 一緒に遊ぼうよ！\n",
            "タウ： え、いいんですか?\n",
            "----------------------------------------------------------\n",
            "細野： 世の中がお飯事に見えてやる気が失せます。\n",
            "タウ： もうお昼は、お昼です。\n",
            "----------------------------------------------------------\n",
            "細野： 最近幸せなんだ〜。\n",
            "タウ： え、私も幸せだよ\n",
            "----------------------------------------------------------\n",
            "細野： ディズニーランドより、東急ハンズが好きなんです。\n",
            "タウ： ご愁傷さまです。\n",
            "----------------------------------------------------------\n",
            "細野： レイヤーバーワンダーバーワンってなんですか？\n",
            "タウ： これです。。\n",
            "----------------------------------------------------------\n",
            "細野： 東急ハンズが好きです。\n",
            "タウ： 3時間くらい寝てた\n",
            "----------------------------------------------------------\n",
            "細野： 東急ハンズで買った傘立てがお気に入りです。\n",
            "タウ： 今日は暑くて暑いです。\n",
            "----------------------------------------------------------\n",
            "細野： FF外失。処理的には可能だと思います。\n",
            "タウ： そうです。今、3時からですかね?\n",
            "----------------------------------------------------------\n",
            "細野： 動揺しすぎです。ちゃんと勉強しましょうね。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： ベジータはとある惑星の王子です。\n",
            "タウ： おめでとー!\n",
            "----------------------------------------------------------\n",
            "細野： お前あたおかなんか。\n",
            "タウ： ただいまです。。\n",
            "----------------------------------------------------------\n",
            "細野： 豆腐が好きです。\n",
            "タウ： はやくは、くっくさん、おめでと。\n",
            "----------------------------------------------------------\n",
            "細野： 豆腐がすきです。\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： 豆腐がスキです。\n",
            "タウ： はやくは、くっくりー。\n",
            "----------------------------------------------------------\n",
            "細野： オレ、トウフ、スキ\n",
            "タウ： おめでとです。\n",
            "----------------------------------------------------------\n",
            "細野： 泣いて馬謖を斬れよ。\n",
            "タウ： とても素敵です。。\n",
            "----------------------------------------------------------\n",
            "細野： 斬撃。一撃。\n",
            "タウ： 大丈夫ですか?。。\n",
            "----------------------------------------------------------\n",
            "細野： \n",
            "タウ： 大丈夫です!ただ、本家で行きます。\n",
            "----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNLPhpZsRz_a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}