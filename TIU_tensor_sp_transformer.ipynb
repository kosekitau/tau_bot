{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TIU_tensor_sp_transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hutiLlgFAlu"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pipefjH6Fw4P"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsgjUDF4FzOo",
        "outputId": "dfa20186-84ea-4708-e5f5-b5acae409812"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oed7D13zF2qN"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/dataset/TIU/twitter/sentencepiece/original1217_train.csv\", header=None)\n",
        "tiu_input = df[0].to_list()\n",
        "tiu_output = df[1].to_list()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIyKhrFUF89U",
        "outputId": "1c3e2200-19a7-4fde-cefe-bfca6bdafcb7"
      },
      "source": [
        "tiu_input[0:2]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['なwwwにwwwこwwwれwww', '日本語上手ですね? 学べましたか?。']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "9CO8uqOAITh3",
        "outputId": "6c5f1684-dc43-4a0a-eb36-3e367cde8628"
      },
      "source": [
        "\"\"\"\n",
        "SentencepieceTokenizerを使う場合\n",
        "!pip install tensorflow_text\n",
        "from tensorflow_text import  SentencepieceTokenizer\n",
        "mod = tf.io.gfile.GFile(\"/content/drive/MyDrive/dataset/TIU/twitter/sentencepiece/sp_tiu1217.model\", 'rb').read()\n",
        "tokenizer = SentencepieceTokenizer(\n",
        "    model=mod,\n",
        "    out_type=tf.string,\n",
        "    add_bos=True, \n",
        "    add_eos=True\n",
        ")\n",
        "tokenizer.tokenize([\"朝青龍になりたい\"])\n",
        "\"\"\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nSentencepieceTokenizerを使う場合\\n!pip install tensorflow_text\\nfrom tensorflow_text import  SentencepieceTokenizer\\nmod = tf.io.gfile.GFile(\"/content/drive/MyDrive/dataset/TIU/twitter/sentencepiece/sp_tiu1217.model\", \\'rb\\').read()\\ntokenizer = SentencepieceTokenizer(\\n    model=mod,\\n    out_type=tf.string,\\n    add_bos=True, \\n    add_eos=True\\n)\\ntokenizer.tokenize([\"朝青龍になりたい\"])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOrMmpJbOMW4",
        "outputId": "a34e4ad9-e08a-44c8-f9a9-a2ff62d5b2e8"
      },
      "source": [
        "!pip install sentencepiece > /dev/null\n",
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"/content/drive/MyDrive/dataset/TIU/twitter/sentencepiece/sp_tiu1217.model\")\n",
        "sp.encode(\"朝青龍になりたい\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 487, 945, 2939, 4055]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSfmexedMVa0",
        "outputId": "35002293-02c2-4957-fb26-0a326f0c94bd"
      },
      "source": [
        "print(sp.get_piece_size())\n",
        "sp.id_to_piece([0,1,2,3,4,5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<s>', '</s>', '。', ':', '、']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5RrStcwMEXV"
      },
      "source": [
        "VOCAB_SIZE_INPUT=sp.get_piece_size()\n",
        "VOCAB_SIZE_OUTPUT=sp.get_piece_size()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRqqN3o2Iq-j"
      },
      "source": [
        "inputs = [[1] + sp.encode(sentence) + [2]\n",
        "          for sentence in tiu_input]\n",
        "outputs = [[1] + sp.encode(sentence) + [2]\n",
        "          for sentence in tiu_output]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeedj3KGOYtO",
        "outputId": "bc400742-1d08-46da-ed67-b9102383f456"
      },
      "source": [
        "print(len(inputs), len(outputs))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158351 158351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTaFxJkeOkIP",
        "outputId": "bc073540-8ae9-437f-913d-cce74fc99a7d"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent)>MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent)>MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "  \n",
        "print(len(inputs), len(outputs))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "133915 133915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov8iJyHfPnWr"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f15fzH9LQFg8"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVnLnPKuQMo4"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "  def get_angles(self, pos, i, d_model):\n",
        "    angles = 1/np.power(10000., (2*(i//2))/np.float32(d_model)  )\n",
        "    return pos*angles #[seq_length, d_model]\n",
        "\n",
        "  def call(self, inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = inputs.shape.as_list()[-1]\n",
        "    angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis, :],\n",
        "                             d_model)\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    pos_encoding = angles[np.newaxis, ...]\n",
        "    return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrZXOv_8aJN3"
      },
      "source": [
        "def scaled_dot_product_attention(quaries, keys, values, mask):\n",
        "  product = tf.matmul(quaries, keys, transpose_b=True)\n",
        "\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask*-1e9)\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values  )\n",
        "\n",
        "  return attention"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixo1dEBSaLYz"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "    self.quary_lin = layers.Dense(units=self.d_model)\n",
        "    self.key_lin = layers.Dense(units=self.d_model)\n",
        "    self.value_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "  def split_proj(self, inputs, batch_size): # inputs: [batch_size, seq_length, nb_proj, d_proj]\n",
        "    shape = (batch_size,\n",
        "             -1,\n",
        "             self.nb_proj,\n",
        "             self.d_proj)\n",
        "    splited_inputs = tf.reshape(inputs, shape=shape) #[batch_size, seq_length, nb_proj, d_proj]\n",
        "    return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) #[batch_size, nb_proj, seq_length, d_proj]\n",
        "\n",
        "  def call(self, quaries, keys, values, mask):\n",
        "    batch_size = tf.shape(quaries)[0]\n",
        "\n",
        "    quaries = self.quary_lin(quaries)\n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values)\n",
        "\n",
        "    quaries = self.split_proj(quaries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)\n",
        "    values = self.split_proj(values, batch_size)\n",
        "\n",
        "    attention = scaled_dot_product_attention(quaries, keys, values, mask)\n",
        "    attention = tf.transpose(attention, perm=[0,2,1,3])\n",
        "    concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "    outputs = self.final_lin(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qioHDY7SaNQY"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "  def __init__(self, FFN_units, nb_proj, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    attention = self.multi_head_attention(inputs,\n",
        "                                          inputs,\n",
        "                                          inputs,\n",
        "                                          mask)\n",
        "    attention = self.dropout_1(attention, training=training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    outputs = self.dense_1(attention)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_2(outputs)\n",
        "    outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwxHAHlBaPir"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout, \n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name='encoder'):\n",
        "    super(Encoder, self).__init__(name=name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout)\n",
        "    self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                   nb_proj,\n",
        "                                   dropout)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outpus = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk1gIwflaRp_"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "  def __init__(self, FFN_units, nb_proj, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    \n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense_1  = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "  \n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    attention = self.multi_head_attention_1(inputs, \n",
        "                                           inputs,\n",
        "                                           inputs,\n",
        "                                           mask_1)\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.norm_1(attention+inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,\n",
        "                                              enc_outputs,\n",
        "                                              enc_outputs,\n",
        "                                              mask_2)\n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "    outputs= self.dense_1(attention_2)\n",
        "    outputs= self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.norm_3(outputs+attention_2)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOQQNvdbaTyL"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "  def __init__(self, nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name='decoder'):\n",
        "    super(Decoder, self).__init__(name=name)\n",
        "    self.d_model=d_model\n",
        "    self.nb_layers = nb_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                   nb_proj,\n",
        "                                   dropout)\n",
        "                        for _ in range(nb_layers)]\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) \n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs,\n",
        "                                  enc_outputs,\n",
        "                                  mask_1,\n",
        "                                  mask_2,\n",
        "                                  training)\n",
        "    return outputs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCh4vkKSaVos"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, \n",
        "               vocab_size_enc,\n",
        "               vocab_size_dec,\n",
        "               d_model,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               name='transformer'):\n",
        "    super(Transformer, self).__init__(name=name)\n",
        "\n",
        "    self.encoder = Encoder(nb_layers,\n",
        "                           FFN_units,\n",
        "                           nb_proj,\n",
        "                           dropout,\n",
        "                           vocab_size_enc,\n",
        "                           d_model)\n",
        "    self.decoder = Decoder(nb_layers,\n",
        "                           FFN_units,\n",
        "                           nb_proj,\n",
        "                           dropout,\n",
        "                           vocab_size_dec,\n",
        "                           d_model)\n",
        "    self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "  \n",
        "  def create_padding_mask(self, seq): #seq:[batch_size, seq_length]\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  def create_look_ahead_mask(self, seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask\n",
        "\n",
        "  def call(self, enc_inputs, dec_inputs, training):\n",
        "    enc_mask = self.create_padding_mask(enc_inputs)\n",
        "    dec_mask_1 = tf.maximum(\n",
        "        self.create_padding_mask(dec_inputs),\n",
        "        self.create_look_ahead_mask(dec_inputs)\n",
        "    )\n",
        "    dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "    enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "    dec_outputs = self.decoder(dec_inputs, \n",
        "                               enc_outputs,\n",
        "                               dec_mask_1,\n",
        "                               dec_mask_2,\n",
        "                               training)\n",
        "    outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcXPdbypaXnV",
        "outputId": "15e40c76-32d1-4748-c174-57c6f4ae79b8"
      },
      "source": [
        "def create_padding_mask(seq): #seq:[batch_size, seq_length]\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask\n",
        "seq = tf.cast([[837,836,0,273,0,0,0,0]], tf.int32)\n",
        "tf.maximum(create_padding_mask(seq), create_look_ahead_mask(seq))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 8, 8), dtype=float32, numpy=\n",
              "array([[[[0., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 0., 1., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyR895eyaarK"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "D_MODEL = 128 #512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_INPUT,\n",
        "                          vocab_size_dec = VOCAB_SIZE_OUTPUT,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM7pOLbRac8Z"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "  loss_ = loss_object(target, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *=mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY6ymvuoahu7"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model=tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step*(self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                    beta_1=0.9,\n",
        "                                    beta_2=0.98,\n",
        "                                    epsilon=1e-9)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4zfmC2Rajv7"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/model/TIU/tensor_sp_transformer/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest checkpoint restored\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_5M21nxa09_",
        "outputId": "e0815068-4e65-497d-c08b-b751cac2eee2"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Start number of epoch {}\".format(epoch+1))\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "    dec_inputs = targets[:, :-1]\n",
        "    dec_outputs_real = targets[:, 1:]\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "      loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    #optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    #WARNING回避で追加\n",
        "    optimizer.apply_gradients((grad, var) for (grad, var) in zip(gradients, transformer.trainable_variables) if grad is not None)\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "          epoch+1, batch, train_loss.result(), train_accuracy.result()\n",
        "      ))\n",
        "\n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "\n",
        "  print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start number of epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.5891 Accuracy 0.0000\n",
            "Epoch 1 Batch 100 Loss 5.4899 Accuracy 0.0337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2PrQ-LpbDX6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}